{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0455df4b-0789-4fbc-942a-95edd780d5c6",
   "metadata": {},
   "source": [
    "# Use Optuna to explore our CatBoost hyperparameter space and efficiently drive validation RMSE down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e818f4-eecf-4482-b354-cc4d1a23197d",
   "metadata": {},
   "source": [
    "### Imports & data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e4f8971f-1deb-4f63-af67-1d7b9b9ba0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import optuna\n",
    "\n",
    "# 1) Load your preprocessed splits\n",
    "X_train, y_train = joblib.load(\"data/processed/train.pkl\")\n",
    "X_val,   y_val   = joblib.load(\"data/processed/val.pkl\")\n",
    "X_test,  y_test  = joblib.load(\"data/processed/test.pkl\")\n",
    "\n",
    "# Optionally merge train+val later:\n",
    "X_train_full = np.vstack([X_train, X_val])\n",
    "y_train_full = np.concatenate([y_train, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a51859-f77c-46ad-a4a4-eb480d88d1b2",
   "metadata": {},
   "source": [
    "### Baseline CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d0e6d5b3-0010-4129-a9ca-027bb0039d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÅ Baseline CatBoost\n",
      "‚è± Train time     : 13.73s\n",
      "üîé Test RMSE      : 5,826.89\n",
      "üìà Test R¬≤        : 0.940\n",
      "‚ö°Ô∏è Predict time   : 1.354s for 43165 samples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.07,        # a strong default\n",
    "    depth=8,\n",
    "    random_seed=42,\n",
    "    loss_function=\"RMSE\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# train with early stopping on VAL\n",
    "t0 = time.time()\n",
    "baseline.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=(X_val, y_val),\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "baseline_time = time.time() - t0\n",
    "\n",
    "# inference + metrics\n",
    "t1 = time.time()\n",
    "y_pred = baseline.predict(X_test)\n",
    "pred_time = time.time() - t1\n",
    "\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "test_r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"üèÅ Baseline CatBoost\")\n",
    "print(f\"‚è± Train time     : {baseline_time:.2f}s\")\n",
    "print(f\"üîé Test RMSE      : {test_rmse:,.2f}\")\n",
    "print(f\"üìà Test R¬≤        : {test_r2:.3f}\")\n",
    "print(f\"‚ö°Ô∏è Predict time   : {pred_time:.3f}s for {len(X_test)} samples\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2658de3-6ed2-406d-818c-51dfe8b86877",
   "metadata": {},
   "source": [
    "The out-of-the-box CatBoostRegressor (LR=0.07, depth = 8) already outperforms our tuned XGBoost/LightGBM baselines, achieving an R¬≤ of 0.94 and bringing error down to about $5.8k "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b738f0-8a74-437d-9337-3e0c91c6bc6f",
   "metadata": {},
   "source": [
    "### Optuna tuning around that sweet spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b5fb39e-be42-40df-8cbd-3b415018ad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-24 20:25:44,191] A new study created in memory with name: no-name-2be0363f-25cf-43b6-8912-60b9193b542a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7eb2a078ed4fea9f013f4f7c56f6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-05-24 20:26:09,298] Trial 0 finished with value: 1286.7508224274256 and parameters: {'learning_rate': 0.10478777287244835, 'depth': 7, 'subsample': 0.6330673671473559, 'colsample_bylevel': 0.7545190672922635}. Best is trial 0 with value: 1286.7508224274256.\n",
      "[I 2025-05-24 20:26:50,501] Trial 1 finished with value: 2448.0141631667043 and parameters: {'learning_rate': 0.04251427298249675, 'depth': 7, 'subsample': 0.9308164973534913, 'colsample_bylevel': 0.6242298001090923}. Best is trial 0 with value: 1286.7508224274256.\n",
      "[I 2025-05-24 20:28:13,852] Trial 2 finished with value: 1686.8910284603305 and parameters: {'learning_rate': 0.03288859795526015, 'depth': 10, 'subsample': 0.8647433809800993, 'colsample_bylevel': 0.8200377304336339}. Best is trial 0 with value: 1286.7508224274256.\n",
      "[I 2025-05-24 20:28:36,239] Trial 3 finished with value: 1624.9710761983627 and parameters: {'learning_rate': 0.10815961329900999, 'depth': 6, 'subsample': 0.7990106721294418, 'colsample_bylevel': 0.8390163467384881}. Best is trial 0 with value: 1286.7508224274256.\n",
      "[I 2025-05-24 20:28:57,583] Trial 4 finished with value: 1415.1839165612384 and parameters: {'learning_rate': 0.0662974571536132, 'depth': 8, 'subsample': 0.6190751261357177, 'colsample_bylevel': 0.9060142147313159}. Best is trial 0 with value: 1286.7508224274256.\n",
      "[I 2025-05-24 20:29:14,613] Trial 5 finished with value: 2728.44904000684 and parameters: {'learning_rate': 0.04999159725488528, 'depth': 6, 'subsample': 0.764733380955612, 'colsample_bylevel': 0.9624545071692097}. Best is trial 0 with value: 1286.7508224274256.\n",
      "[I 2025-05-24 20:29:35,064] Trial 6 finished with value: 1233.1727022980333 and parameters: {'learning_rate': 0.11256894441746099, 'depth': 7, 'subsample': 0.8348854787697007, 'colsample_bylevel': 0.782141576145469}. Best is trial 6 with value: 1233.1727022980333.\n",
      "[I 2025-05-24 20:30:16,667] Trial 7 finished with value: 1833.0734577665119 and parameters: {'learning_rate': 0.029682277937339935, 'depth': 10, 'subsample': 0.636455533395369, 'colsample_bylevel': 0.9198238719612224}. Best is trial 6 with value: 1233.1727022980333.\n",
      "[I 2025-05-24 20:30:41,516] Trial 8 finished with value: 880.9587186615107 and parameters: {'learning_rate': 0.09561108831563205, 'depth': 9, 'subsample': 0.6756267540939711, 'colsample_bylevel': 0.8456558619929287}. Best is trial 8 with value: 880.9587186615107.\n",
      "[I 2025-05-24 20:31:01,195] Trial 9 finished with value: 1663.03195437813 and parameters: {'learning_rate': 0.07358175063622521, 'depth': 7, 'subsample': 0.8459350361029985, 'colsample_bylevel': 0.921410346557877}. Best is trial 8 with value: 880.9587186615107.\n",
      "[I 2025-05-24 20:31:26,124] Trial 10 finished with value: 928.0355339417434 and parameters: {'learning_rate': 0.08563240044178916, 'depth': 9, 'subsample': 0.7191123158763333, 'colsample_bylevel': 0.6897783405411959}. Best is trial 8 with value: 880.9587186615107.\n",
      "[I 2025-05-24 20:31:51,555] Trial 11 finished with value: 910.8585590702924 and parameters: {'learning_rate': 0.08780134656507349, 'depth': 9, 'subsample': 0.7199411136216685, 'colsample_bylevel': 0.6661543511669266}. Best is trial 8 with value: 880.9587186615107.\n",
      "[I 2025-05-24 20:32:16,366] Trial 12 finished with value: 886.9766852092537 and parameters: {'learning_rate': 0.08882591391359114, 'depth': 9, 'subsample': 0.7175145625780115, 'colsample_bylevel': 0.6002683926952368}. Best is trial 8 with value: 880.9587186615107.\n",
      "[I 2025-05-24 20:32:40,990] Trial 13 finished with value: 895.2420175720065 and parameters: {'learning_rate': 0.09195856747709756, 'depth': 9, 'subsample': 0.6948169592111089, 'colsample_bylevel': 0.7269463292590732}. Best is trial 8 with value: 880.9587186615107.\n",
      "[I 2025-05-24 20:33:05,169] Trial 14 finished with value: 917.9879718503194 and parameters: {'learning_rate': 0.11971088563738014, 'depth': 8, 'subsample': 0.9991486388615407, 'colsample_bylevel': 0.860288520787658}. Best is trial 8 with value: 880.9587186615107.\n",
      "[I 2025-05-24 20:33:31,526] Trial 15 finished with value: 1266.0375452903552 and parameters: {'learning_rate': 0.06137186959088445, 'depth': 9, 'subsample': 0.6873074524950323, 'colsample_bylevel': 0.6019582097237394}. Best is trial 8 with value: 880.9587186615107.\n",
      "[I 2025-05-24 20:34:12,523] Trial 16 finished with value: 687.0205802635361 and parameters: {'learning_rate': 0.09647536162561265, 'depth': 10, 'subsample': 0.6634736346124851, 'colsample_bylevel': 0.721308543663492}. Best is trial 16 with value: 687.0205802635361.\n",
      "[I 2025-05-24 20:34:52,338] Trial 17 finished with value: 661.6201285348678 and parameters: {'learning_rate': 0.09822572983149677, 'depth': 10, 'subsample': 0.602411659615706, 'colsample_bylevel': 0.7160926387843792}. Best is trial 17 with value: 661.6201285348678.\n",
      "[I 2025-05-24 20:35:31,688] Trial 18 finished with value: 828.2494594284775 and parameters: {'learning_rate': 0.07608021697100269, 'depth': 10, 'subsample': 0.6015968281807457, 'colsample_bylevel': 0.7072682007418765}. Best is trial 17 with value: 661.6201285348678.\n",
      "[I 2025-05-24 20:36:14,515] Trial 19 finished with value: 673.0742840160706 and parameters: {'learning_rate': 0.10066754748798765, 'depth': 10, 'subsample': 0.6538162069861614, 'colsample_bylevel': 0.7727995939631086}. Best is trial 17 with value: 661.6201285348678.\n",
      "[I 2025-05-24 20:36:58,406] Trial 20 finished with value: 638.7996450127762 and parameters: {'learning_rate': 0.10368857993920925, 'depth': 10, 'subsample': 0.7595119953545801, 'colsample_bylevel': 0.7753700535257783}. Best is trial 20 with value: 638.7996450127762.\n",
      "[I 2025-05-24 20:37:46,957] Trial 21 finished with value: 649.3111136003379 and parameters: {'learning_rate': 0.1016041813152126, 'depth': 10, 'subsample': 0.7662695612806177, 'colsample_bylevel': 0.7711121432919922}. Best is trial 20 with value: 638.7996450127762.\n",
      "[I 2025-05-24 20:38:36,879] Trial 22 finished with value: 598.4998796049568 and parameters: {'learning_rate': 0.11396580711779083, 'depth': 10, 'subsample': 0.7666430805237844, 'colsample_bylevel': 0.6625175939396063}. Best is trial 22 with value: 598.4998796049568.\n",
      "[I 2025-05-24 20:39:05,595] Trial 23 finished with value: 921.063599540496 and parameters: {'learning_rate': 0.11904758544669186, 'depth': 8, 'subsample': 0.7742729553733759, 'colsample_bylevel': 0.6643500932533959}. Best is trial 22 with value: 598.4998796049568.\n",
      "[I 2025-05-24 20:39:54,163] Trial 24 finished with value: 594.3636766395668 and parameters: {'learning_rate': 0.11202052684545272, 'depth': 10, 'subsample': 0.7656391272027128, 'colsample_bylevel': 0.7999057188253067}. Best is trial 24 with value: 594.3636766395668.\n",
      "[I 2025-05-24 20:40:44,891] Trial 25 finished with value: 617.0700337183046 and parameters: {'learning_rate': 0.11228507101701357, 'depth': 10, 'subsample': 0.8061982784461201, 'colsample_bylevel': 0.8079345932683148}. Best is trial 24 with value: 594.3636766395668.\n",
      "[I 2025-05-24 20:41:11,025] Trial 26 finished with value: 746.5506896301534 and parameters: {'learning_rate': 0.11186022441754087, 'depth': 9, 'subsample': 0.823345611610477, 'colsample_bylevel': 0.9827272511032735}. Best is trial 24 with value: 594.3636766395668.\n",
      "[I 2025-05-24 20:42:00,852] Trial 27 finished with value: 770.6674047928419 and parameters: {'learning_rate': 0.08208758754323814, 'depth': 10, 'subsample': 0.8935383501297579, 'colsample_bylevel': 0.8797174919081104}. Best is trial 24 with value: 594.3636766395668.\n",
      "[I 2025-05-24 20:42:23,687] Trial 28 finished with value: 940.5899633387268 and parameters: {'learning_rate': 0.1150806215932853, 'depth': 8, 'subsample': 0.7900355654879077, 'colsample_bylevel': 0.8110819511684192}. Best is trial 24 with value: 594.3636766395668.\n",
      "[I 2025-05-24 20:43:07,936] Trial 29 finished with value: 615.3397691279863 and parameters: {'learning_rate': 0.1081645187426907, 'depth': 10, 'subsample': 0.7384938277142951, 'colsample_bylevel': 0.7997554920135693}. Best is trial 24 with value: 594.3636766395668.\n",
      "\n",
      "üîç Best CatBoost trial:\n",
      "  RMSE=594.36\n",
      "  ‚Ä¢ learning_rate        = 0.11202052684545272\n",
      "  ‚Ä¢ depth                = 10\n",
      "  ‚Ä¢ subsample            = 0.7656391272027128\n",
      "  ‚Ä¢ colsample_bylevel    = 0.7999057188253067\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # narrow around 0.02‚Äì0.12 and depth 6‚Äì10\n",
    "    lr    = trial.suggest_float(\"learning_rate\", 0.02, 0.12, log=False)\n",
    "    depth = trial.suggest_int(\"depth\", 6, 10)\n",
    "    subs  = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "    colsm = trial.suggest_float(\"colsample_bylevel\", 0.6, 1.0)\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=2000,             # allow more rounds but early stop\n",
    "        learning_rate=lr,\n",
    "        depth=depth,\n",
    "        subsample=subs,\n",
    "        colsample_bylevel=colsm,\n",
    "        random_seed=42,\n",
    "        loss_function=\"RMSE\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # use Pool to avoid pandas wrappers\n",
    "    train_pool = Pool(X_train_full, y_train_full)\n",
    "    val_pool   = Pool(X_val, y_val)\n",
    "\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,\n",
    "        early_stopping_rounds=50,\n",
    "    )\n",
    "    preds = model.predict(X_val)\n",
    "    return mean_squared_error(y_val, preds, squared=False)  # RMSE\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nüîç Best CatBoost trial:\")\n",
    "best = study.best_trial\n",
    "print(f\"  RMSE={best.value:.2f}\")\n",
    "for k,v in best.params.items():\n",
    "    print(f\"  ‚Ä¢ {k:<20} = {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252aa444-af8d-4ec7-b390-585489f0a72f",
   "metadata": {},
   "source": [
    "Optuna‚Äôs best search on the validation split finds a sharper configuration (notably a higher learning rate and deeper trees). This single‚Äêtrial val‚ÄêRMSE (~594) hints at far lower error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181251e-a8d3-4e2f-9eeb-a260a245c394",
   "metadata": {},
   "source": [
    "### Re-fit on Train+Val with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c742fadc-600e-4c16-8bda-e0c42e3473ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best.params.copy()\n",
    "best_params.update({\n",
    "    \"iterations\": 2000,\n",
    "    \"loss_function\": \"RMSE\",\n",
    "    \"random_seed\": 42,\n",
    "    \"verbose\": False\n",
    "})\n",
    "\n",
    "final_model = CatBoostRegressor(**best_params)\n",
    "\n",
    "t2 = time.time()\n",
    "final_model.fit(\n",
    "    X_train_full, y_train_full,\n",
    "    eval_set=(X_val, y_val),\n",
    "    early_stopping_rounds=50,\n",
    ")\n",
    "final_time = time.time() - t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f058251-d22e-4d8c-bac2-7a655fb6134b",
   "metadata": {},
   "source": [
    "### Final evaluation on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb798770-8e98-4f32-b0ba-b2695e679a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = time.time()\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "final_pred_time = time.time() - t3\n",
    "\n",
    "final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "final_r2   = r2_score(y_test, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0ef30342-88ae-4aaf-9b8d-0b63c9c02cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Tuned CatBoost + Optuna\n",
      "‚è± Train time     : 44.32s\n",
      "üîé Test RMSE      : 5,194.53\n",
      "üìà Test R¬≤        : 0.952\n",
      "‚ö°Ô∏è Predict time   : 1.348s for 43165 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüèÜ Tuned CatBoost + Optuna\")\n",
    "print(f\"‚è± Train time     : {final_time:.2f}s\")\n",
    "print(f\"üîé Test RMSE      : {final_rmse:,.2f}\")\n",
    "print(f\"üìà Test R¬≤        : {final_r2:.3f}\")\n",
    "print(f\"‚ö°Ô∏è Predict time   : {final_pred_time:.3f}s for {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa7b75-c08a-4f17-8403-5137a83946f3",
   "metadata": {},
   "source": [
    "After refitting on train + val with the top Optuna parameters, CatBoost drops test RMSE to ~$5.2k and raises R¬≤ to 0.952‚Äîan additional 11% error reduction from the baseline. This makes it our new best model overall:\n",
    "\n",
    "‚úîÔ∏è Better generalization vs. baseline CatBoost (R¬≤ +0.012)\n",
    "\n",
    "‚úîÔ∏è Stronger performance vs. tuned XGBoost/LightGBM/NeuralNets\n",
    "\n",
    "‚ö° Inference speed unchanged (~1.35 s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2b1a49-7188-433f-b3f6-b9e9295daf06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
